\section{Results}
We evaluated the performance of three LLMs (GPT-o1, Claude, and Grok) in generating accurate SQL queries for the Cantus Database, comparing two scenarios: with and without a schema's predefined options. Results are reported as counts out of 45 for correct values with and without ordering, along with precision, recall, and F1 scores. Findings are shown in \textit{Table \ref{tab:schema_without_data}} and \textit{Table \ref{tab:schema_with_data}}.

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Metric}             & \textbf{GPT} & \textbf{Claude} & \textbf{Grok}   \\
\hline
Unordered (count)                 & \textbf{23}  & \textbf{23}     & 16              \\
Ordered (count)                      & \textbf{16}  & 15              & 11              \\
Precision                   & 0.70        & \textbf{0.75}          & 0.56  \\
Recall                      & 0.69        & \textbf{0.74}  & 0.47           \\
F1                          & 0.68 &    \textbf{0.70}      & 0.48           \\
\hline
\end{tabular}
\caption{Results for the schema without options. The unordered and ordered metrics display counts out of 45.}
\label{tab:schema_without_data}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Metric}             & \textbf{GPT} & \textbf{Claude} & \textbf{Grok}   \\
\hline
Unordered (count)                     & \textbf{32}  & 28              & 23              \\
Ordered (count)                     & \textbf{23}  & 16              & 10              \\
Precision                   & \textbf{0.92}       & 0.85          & 0.69  \\
Recall                      & \textbf{0.88} & 0.78          & 0.60           \\
F1                          & \textbf{0.88} & 0.78          & 0.61           \\
\hline
\end{tabular}
\caption{Results for the schema with options. The unordered and ordered metrics display counts out of 45.}
\label{tab:schema_with_data}
\end{table}