\section{Related Work}

Generating SQL queries from natural language is a widely studied task in natural language processing. Previous research demonstrates that scaling training data and model size in generative models enables advanced functionalities, such as few-shot learning, without task-specific fine-tuning \cite{rajkumar2022texttosql}. Notably, the work shows that models like Codex and GPT-3 perform competitively as Text-to-SQL solutions without additional fine-tuning. The study also explores the impact of varied prompts and assesses the performance of multiple models on established datasets, such as Spider, to determine cross-domain Text-to-SQL capabilities.

Other work investigates prompt engineering strategies for LLMs in Text-to-SQL tasks \cite{gao2023text2sql}. This research systematically evaluates techniques such as question representation, in-context learning, and supervised fine-tuning on benchmark datasets like Spider and Spider-Realistic. They propose a novel solution, DAIL-SQL, which leverages these strategies to enhance Text-to-SQL performance. The authors also provide an empirical analysis of zero-shot scenarios, example selection, and organization strategies in few-shot learning, highlighting the potential of open-source LLMs for both in-context learning and fine-tuning.

Building on the widespread use of the Spider dataset in Text-to-SQL research, a recent study further investigated strategies for leveraging LLMs for this task \cite{roberson2024analyzing}. Two main approaches were examined: fine-tuning open-source models and using closed-source models with advanced techniques. They identified seven common error types in generated queries, such as incorrect column selection, grouping errors, and faulty JOIN clauses. While closed-source models excelled in few-shot in-context learning, the authors noted that some queries flagged as incorrect still produce correct outputs, highlighting the need for improved evaluation metrics in Text-to-SQL research.

While these works provide insights into the capabilities of LLMs for Text-to-SQL tasks, they primarily focus on established datasets like Spider, which are larger and may have been exposed to the evaluated LLMs during training, potentially introducing overfitting. In contrast, in this study we employ a custom-made, domain-specific dataset derived from Cantus Database. Our research avoids fine-tuning or supervised learning by exclusively using prompt engineering to evaluate pre-trained LLMs. In addition, our work tested several prompt engineering strategies before selecting and standardizing those used in the JSON data schema. By evaluating multiple LLMs and optimizing prompt strategies, this work demonstrates the adaptability of LLMs to specialized datasets and highlights prompt engineering as an effective, lightweight approach for domain-specific Text-to-SQL tasks.