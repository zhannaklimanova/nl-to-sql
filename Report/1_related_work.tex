\section{Related Work}

% Related work: Summarize previous work in the area that you have found. I don't expect you to give a complete and fully up-to-date survey of related work in the area. Instead, aim to cite and discuss at least 3-5 relevant papers, with a focus on how your work differs from theirs.

Generating SQL queries from natural language has become a critical task in natural language processing, driven by the capabilities of large language models (LLMs) to capture complex patterns within a provided database schema. The work presented in this paper shares similarities with an older study from 2022 conducted in \cite{rajkumar2022texttosql}, which highlighted that scaling training data and model size in generative language models enables advanced functionalities, such as few-shot learning without the need for task-specific finetuning. Notably, the work in \cite{rajkumar2022texttosql} demonstrates that models like Codex and GPT-3 serve as competitive Text-to-SQL solutions even without additional finetuning, a finding that aligns with this study's use of OpenAI's newer models for similar purposes. However, there are key differences. Unlike this paper, which evaluates a consistent natural language query prompt across varied data types, the work in \cite{rajkumar2022texttosql} explores varied prompts and benchmarks the performance of models of different sizes, including Codex and GPT-3, on established datasets like Spider for cross-domain Text-to-SQL tasks. While this paper introduces and evaluates a novel dataset created specifically for this project, \cite{rajkumar2022texttosql} conducted experiments in few-shot settings using benchmarks such as GeoQuery and Scholar. These differences underscore the complementary nature of the two studies in advancing the evaluation of LLMs for Text-to-SQL tasks.

Recent work by \cite{gao2023text2sql} investigates various prompt engineering strategies for open-source large language models (LLMs) in Text-to-SQL tasks. Their study systematically evaluates prompt engineering techniques, including question representation, in-context learning, and supervised fine-tuning, on benchmark datasets such as Spider and Spider-Realistic. They propose a novel solution, DAIL-SQL, which leverages these strategies to enhance the effectiveness and efficiency of Text-to-SQL performance. Unlike \cite{gao2023text2sql}, this project focuses on evaluating LLMs on a smaller, custom-built dataset derived from the Cantus Database. The primary aim is to explore how pre-trained LLMs identify patterns in the data without relying on supervised fine-tuning, keeping the data and prompts consistent. While both studies emphasize prompt engineering, they differ in scope and methodology. \cite{gao2023text2sql} provides an extensive empirical analysis of prompt engineering strategies, including zero-shot scenarios, example selection, and organization strategies in few-shot learning. Their research also highlights the potential of open-source LLMs in both in-context learning and supervised fine-tuning. In contrast, this study utilizes primarily closed-source, proprietary LLMs, focusing on their pre-trained capabilities. Like \cite{gao2023text2sql}, this study manually tested several prompt engineering methods before selecting and standardizing those used in the data.json files. However, the goal in \cite{gao2023text2sql} is to compare the performance of various LLMs rather than systematically optimizing prompt engineering strategies.

The most recent work by \cite{zetterman2024text2sql}, conducted in 2024, explores alternatives to OpenAI's GPT-3.5 and GPT-4 for Text-to-SQL tasks. This research compares Claude Opus, an advanced proprietary LLM, with a fine-tuned Mixtral 8x7B, an open-source model. Both models are evaluated on the BIRT dataset using execution accuracy as the primary metric, which involves comparing the resulting rows from executed SQL queries against ground-truth outputs. This approach provides a practical measure of model performance by assessing how well the generated SQL captures the intended query semantics. Zetterman’s work shares similarities with this study in its emphasis on evaluating the quality of generated SQL by comparing the output data to golden standards. However, while Zetterman utilizes the BIRT benchmark dataset, this project evaluates models on a smaller, custom-built dataset derived from the Cantus Database, which focuses on historical and musicological data. Additionally, Zetterman employs fine-tuning on smaller LLMs, such as Mixtral, to enhance domain-specific performance, whereas this study relies exclusively on pre-trained capabilities of both open and closed-source LLMs without performing any fine-tuning. Furthermore, Zetterman’s thesis investigates the practical trade-offs between proprietary and open-source models, emphasizing resource efficiency and accessibility. In contrast, this study prioritizes the robustness of pre-trained models when exposed to promp-engineering techniques, particularly focusing on the pre-trained model's ability to quickly find pattern in the domain-specific dataset. While both works assess execution accuracy, Zetterman’s focus on large-scale, standardized datasets contrasts with this study’s exploration of a niche, highly specialized dataset to understand LLM behavior in specific contexts.


In conclusion, while the works by \cite{rajkumar2022texttosql}, \cite{gao2023text2sql}, and \cite{zetterman2024text2sql} offer valuable insights into the capabilities of large language models (LLMs) for Text-to-SQL tasks, they each operate in contexts distinct from this study. None of these works explore Grok, the specific LLM employed in this project, which represents a novel angle of investigation. Additionally, these studies focus on established datasets like Spider, GeoQuery, Scholar, and BIRT, which are significantly larger and have likely been exposed to LLMs during pretraining or finetuning stages. In contrast, this study introduces a custom-made dataset derived from the Cantus Database, which is specialized, domain-specific, and previously unused in the training, pretraining, or finetuning of LLMs. By relying solely on prompt engineering to evaluate pre-trained LLMs, this work avoids finetuning or supervised learning, focusing instead on leveraging the models' inherent capabilities for inference. This approach not only highlights the adaptability of LLMs to unique datasets but also emphasizes the potential of prompt engineering as a lightweight yet effective strategy for extracting meaningful performance in domain-specific Text-to-SQL tasks.

PROOFREAD AND IMPROVE (LUCAS, CHARLES, ZHANNA)